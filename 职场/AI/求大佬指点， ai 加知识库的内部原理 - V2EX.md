---
link: https://www.v2ex.com/t/1112813
site: V2EX
date: 2025-02-20T09:26
excerpt: 程序员 - @oldManNewThought - 类似 cherry studio, dify 等软件可以使用第三方 ai 的 api
  接口，然后在提供上传文档变成知识库，内部的实现原理是什么？求大佬指点
twitter: https://twitter.com/@V2EX
slurped: 2025-02-20T16:16
title: 求大佬指点， ai 加知识库的内部原理 - V2EX
---
  
  [oldManNewThought](/member/oldManNewThought) · 6 小时 47 分钟前 · 2523 次点击

类似 cherry studio, dify 等软件可以使用第三方 ai 的 api 接口，然后在提供上传文档变成知识库，内部的实现原理是什么？求大佬指点

第 1 条附言  ·  6 小时 11 分钟前

看大家大多都说是，向量数据库加上下文。那 cherry studio 等工具里要求的必须是嵌入模型又是为什么？ deepseek 算嵌入模型吗？我看腾讯的 ima.copilot 这款软件里知识库是支持 deepseek 的。另外有 v2exer 感觉用这种上下文会耗大量 token,这个确实是个问题啊。


30 条回复  **•**  2025-02-20 16:08:48 +08:00

|   |   |   |
|---|---|---|
|![xiadengmaX1](https://cdn.v2ex.com/avatar/5ee2/8ad3/483593_normal.png?m=1657000421)||1<br><br>**[xiadengmaX1](/member/xiadengmaX1)**  <br><br>   6 小时 43 分钟前<br><br>[https://docs.cherry-ai.com/knowledge-base/knowledge-base](https://docs.cherry-ai.com/knowledge-base/knowledge-base)|

|   |   |   |
|---|---|---|
|![superBIUBIU](https://cdn.v2ex.com/gravatar/65332d1319929844abf799bce0023275?s=48&d=retro)||2<br><br>**[superBIUBIU](/member/superBIUBIU)**  <br><br>   6 小时 39 分钟前<br><br>原理就是解析文件形成上下文记忆啊|

|                                                                                        |     |                                                                                                                                                                                                                                                                                                                                                                                                    |
| -------------------------------------------------------------------------------------- | --- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ![visper](https://cdn.v2ex.com/gravatar/838db077a2334ed6bcbc29c29ab41a6e?s=48&d=retro) |     | 3<br><br>**[visper](/member/visper)**  <br><br>   6 小时 38 分钟前<br><br>首先，我们知道，和大模型聊天的时候，我们可以设计 prompt,例如:  <br>请结合以下上下文内容，回答用户问题:  <br>```  <br>context  <br>```  <br>用户问题如下：  <br>query  <br>  <br>-----  <br>**那么，上传文档变成知识库，就是把文档分段，使用向量数据库存起来。当用户提问的时候(假设内容叫 query)，拿 query 内容通过向量搜索找到相关的知识片段,组合成 context,拼到上面类似的 prompt 里面一起给大模型回答。  <br>另外，向量数据库搜索只是比较通用的一种方式。知识怎么分段怎么存，你可以自己设计比如存关系数据库都没问题。** |

|                                                                                 |     |                                                                                                                                                                                                                                                       |
| ------------------------------------------------------------------------------- | --- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ![ihainan](https://cdn.v2ex.com/avatar/2127/7509/76542_normal.png?m=1737512032) |     | 4<br><br>**[ihainan](/member/ihainan)**  <br><br>   6 小时 38 分钟前   ![❤️](/static/img/heart_neue_red.png?v=16ec2dd0a880be6edda1e4a2e35754b3) 1<br><br>**最简单来说，就是文档切片转为向量存入向量数据库，Query 也转为向量，从向量库找出最相似的切分，文本拼接起来给 LLM 模型回答问题。**  <br>  <br>当然实际肯定比这个要复杂。 |

|                                                                                   |     |                                                                                                                                                                                                                                                                                                                                         |
| --------------------------------------------------------------------------------- | --- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ![gavin6liu](https://cdn.v2ex.com/avatar/0bbf/479b/39003_normal.png?m=1497578762) |     | 5<br><br>**[gavin6liu](/member/gavin6liu)**  <br><br>   6 小时 38 分钟前   ![❤️](/static/img/heart_neue_red.png?v=16ec2dd0a880be6edda1e4a2e35754b3) 2<br><br>把你提供的文档 根据一定的规则拆成片段，通过一个模型（ embedding 模型）转成向量，存起来就变成了知识库。这个过程类比 es 一类的搜索引擎，只不过常规搜索引擎是分词，他这个是向量。  <br>  <br>当你问了一个问题，**把你的问题也转成向量，和知识库里面的数据做向量计算，拿到最相似的**，拼到上下文中，发给大模型，让大模型回答给你。 |

|   |   |   |
|---|---|---|
|![cheng6563](https://cdn.v2ex.com/avatar/382f/1f51/414567_normal.png?m=1691026308)||6<br><br>**[cheng6563](/member/cheng6563)**  <br><br>   6 小时 30 分钟前<br><br>和搜索引擎差不多|

|   |   |   |
|---|---|---|
|![cheng6563](https://cdn.v2ex.com/avatar/382f/1f51/414567_normal.png?m=1691026308)||7<br><br>**[cheng6563](/member/cheng6563)**  <br><br>   6 小时 29 分钟前<br><br>尤你的 AI 前端程序（不是模型）在知识库搜索内容，然后插到聊天记录里面|

|   |   |   |
|---|---|---|
|![gaobh](https://cdn.v2ex.com/avatar/4a85/8bdd/205723_normal.png?m=1738847557)||8<br><br>**[gaobh](/member/gaobh)**  <br><br>   6 小时 25 分钟前 via iPhone<br><br>矢量数据库，调好参数把数据扔进去解析，聊天的时候触发关键字去矢量数据库拿命中数据添加到上下文再聊天，好不好用全靠参数调的好不好|

|   |   |   |
|---|---|---|
|![musi](https://cdn.v2ex.com/avatar/3c30/8c17/303588_normal.png?m=1691313864)||9<br><br>**[musi](/member/musi)**  <br><br>   6 小时 23 分钟前<br><br>就是一个搜索引擎，从你的文档中选取和你问题最相关的段落连带着你的问题发给 LLM ，让 LLM 基于文档去回答你的问题。  <br>可以说搜索这部分和 AI 一点关系都没有|

|   |   |   |
|---|---|---|
|![liuchengfeng1](https://cdn.v2ex.com/avatar/445e/d34f/441709_normal.png?m=1732634973)||10<br><br>**[liuchengfeng1](/member/liuchengfeng1)**  <br><br>   6 小时 16 分钟前<br><br>我也想知道；例如我把我博客文章全部给 ai ，让 ai 根据我给它的数据，每次问它能找到对应的文章。该如何实现？最好能封装一个 api|

|   |   |   |
|---|---|---|
|![joyhub2140](https://cdn.v2ex.com/avatar/9f4b/9960/67491_normal.png?m=1715412184)||11<br><br>**[joyhub2140](/member/joyhub2140)**  <br><br>   6 小时 16 分钟前<br><br>模型貌似是无状态的，可以理解外靠外挂向量数据库来构建每一次的提问。  <br>  <br>你每次问之前，外部程序都会先从自己的提问历史结合向量数据库构造出完整的上下文，再打包发给模型。  <br>  <br>我之前还想调用 ollama 的 api ，想着应该会有个 token id 之类的，后面发现没有，想维持上下文，得把之前得提问历史也需要一并发过去给模型，相当于模型只是纯思考的机器，上下文还是得靠外部程序来构建。|

|   |   |   |
|---|---|---|
|![moyufjm123](https://cdn.v2ex.com/gravatar/2e858885c60b885d5d45e1b55604c086?s=48&d=retro)||12<br><br>**[moyufjm123](/member/moyufjm123)**  <br><br>   6 小时 16 分钟前<br><br>小白问一下，那要是知识库很大，上下文很多，token 岂不是消耗很快？应该涉及多次问答会不会重复发送上下文？|

|   |   |   |
|---|---|---|
|![onevcat](https://cdn.v2ex.com/avatar/1509/9585/48749_normal.png?m=1407484797)||13<br><br>**[onevcat](/member/onevcat)**  <br><br>   6 小时 15 分钟前 via iPhone<br><br>RAG|

|   |   |   |
|---|---|---|
|![aloxaf](https://cdn.v2ex.com/gravatar/4ea6081ef8d855cb90225438daf0de63?s=48&d=retro)||14<br><br>**[aloxaf](/member/aloxaf)**  <br><br>   6 小时 2 分钟前<br><br>嵌入模型是用来把文字转成向量的，这样就可以把文字的相似度匹配转成向量的相似度匹配，这样一来更快，二来还能匹配语义上相似但实际不同的句子。|

|   |   |   |
|---|---|---|
|![xwayway](https://cdn.v2ex.com/gravatar/eebbfba0aa7ed95ea7e6cabac96872ab?s=48&d=retro)||15<br><br>**[xwayway](/member/xwayway)**  <br><br>   6 小时 2 分钟前<br><br>@[moyufjm123](/member/moyufjm123) #12 会返回知识库的 top k ，不会整个知识库一起给到大模型|

|   |   |   |
|---|---|---|
|![chairuosen](https://cdn.v2ex.com/avatar/e9c2/83d5/40739_normal.png?m=1371395946)||16<br><br>**[chairuosen](/member/chairuosen)**  <br><br>   6 小时 1 分钟前<br><br>小白再问一下，大语言模型是基于概率预测的，那即使有知识库，它一定能返回和知识库内容完全一样的结果么？比如让它基于条件选择合适的 row 并把知识库这一行完整返回来|

|   |   |   |
|---|---|---|
|![tool2dx](https://cdn.v2ex.com/avatar/c810/399e/684488_normal.png?m=1736062402)||17<br><br>**[tool2dx](/member/tool2dx)**  <br><br>   5 小时 55 分钟前<br><br>@[chairuosen](/member/chairuosen) 我用 deepseek 试过，可以把知识库作为提示词一部分喂给他，基本上回答没啥问题，比别的 AI 模型要聪明。就是比较费 token 。|

|                                                                                        |     |                                                                                                                                                                                                                                                                                                                                                                                                      |
| -------------------------------------------------------------------------------------- | --- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ![aloxaf](https://cdn.v2ex.com/gravatar/4ea6081ef8d855cb90225438daf0de63?s=48&d=retro) |     | 18<br><br>**[aloxaf](/member/aloxaf)**  <br><br>   5 小时 55 分钟前<br><br>@[moyufjm123](/member/moyufjm123)  <br>  <br>> 那要是知识库很大，上下文很多，token 岂不是消耗很快？  <br>  <br>理论上是的，但是一般来讲，只有被匹配到的一段文本（和它们的上下文）会被发送给 LLM ，而且很多工具只会发送匹配度最高的 N 段文本，当然代价就是准确性降低。  <br>  <br>> 涉及多次问答会不会重复发送上下文？  <br>  <br>**会，但有的模型是支持缓存的，比如 OpenAI 和 DeepSeek 连续对话时会自动缓存上下文，命中缓存就有折扣。不过也有模型比如 Claude 和 Gemini 得手动触发缓存，很多工具压根就没有适配……** |

|   |   |   |
|---|---|---|
|![tool2dx](https://cdn.v2ex.com/avatar/c810/399e/684488_normal.png?m=1736062402)||19<br><br>**[tool2dx](/member/tool2dx)**  <br><br>   5 小时 52 分钟前<br><br>@[moyufjm123](/member/moyufjm123) 如果知识库很大，会改用微调大模型，这方法不合适。这方法就是很消耗 token ，没办法。|

|   |   |   |
|---|---|---|
|![crackidz](https://cdn.v2ex.com/gravatar/e5311b443d279d0bce5f7e7606af6c41?s=48&d=retro)||20<br><br>**[crackidz](/member/crackidz)**  <br><br>   5 小时 47 分钟前<br><br>这种基础的问题，你询问 AI 会更快一些...|

|   |   |   |
|---|---|---|
|![moyufjm123](https://cdn.v2ex.com/gravatar/2e858885c60b885d5d45e1b55604c086?s=48&d=retro)||21<br><br>**[moyufjm123](/member/moyufjm123)**  <br><br>   5 小时 47 分钟前<br><br>好的了解，谢谢各位大佬解答|

|                                                                                |     |                                                                                                                                                                                                       |
| ------------------------------------------------------------------------------ | --- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ![TomCN](https://cdn.v2ex.com/avatar/c562/02c5/408895_normal.png?m=1725461381) |     | 22<br><br>**[TomCN](/member/TomCN)**  <br><br>   5 小时 40 分钟前<br><br>**其实知识库的重点应该是文本的切片和向量化，尽可能使查询匹配的结果更精确**  <br>  <br>**低于 token 的消耗，其实很多时候有了精确的匹配之后使用小型的模型也是可以的，而且小模型部署较为简单，也不必去担心持续大量的 token 消耗了** |

|                                                                                      |     |                                                                                                                                                                                                                                                                                     |
| ------------------------------------------------------------------------------------ | --- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ![kenniewwwww](https://cdn.v2ex.com/avatar/5bc8/c7c7/276110_normal.png?m=1714925137) |     | 23<br><br>**[kenniewwwww](/member/kenniewwwww)**  <br><br>   5 小时 32 分钟前<br><br>**这个算是很基础的 RAG 了，粗暴的喂给大模型，其实还可以把文本转化为知识图谱，做图检索，如** [https://arxiv.org/abs/2410.05779](https://arxiv.org/abs/2410.05779) 还有领英这个 [https://arxiv.org/abs/2404.17723](https://arxiv.org/abs/2404.17723) |

|   |   |   |
|---|---|---|
|![yylucian](https://cdn.v2ex.com/avatar/fcba/bd58/58889_normal.png?m=1395656104)||24<br><br>**[yylucian](/member/yylucian)**  <br><br>   4 小时 40 分钟前<br><br>这里涉及两种模型，一种是 embedding 模型（又叫嵌入模型、向量模型等），一种是大语言模型（就是 LLM ，如 deepseek-r1 ）  <br>  <br>原理前面的大佬都解释得很清楚了：  <br>1. 上传知识库，用嵌入模型把知识库内容分词、向量化，存到向量数据库  <br>2. 用户提问时，也调用嵌入模型，把提问内容分词、向量化，和向量数据库的内容进行匹配，找出相关性最高的若干个片段  <br>3. 把用户问题和匹配到的知识内容共同作为提示词，发送给 LLM  <br>4. LLM 基于提供的内容回答问题|

|   |   |   |
|---|---|---|
|![chanlk](https://cdn.v2ex.com/gravatar/34ca350b59c311ae31fb1fda830d6a33?s=48&d=retro)||25<br><br>**[chanlk](/member/chanlk)**  <br><br>   4 小时 36 分钟前<br><br>原理前面的大佬解释的很好了，下面是我从 deepseek 查到的，对普通无 AI 基础的开发更友好的解释：  <br>  <br>用大模型结合用户文档构建问答知识库，核心原理可以用“图书馆+翻译官”的类比来理解，对普通开发者来说主要分三步：  <br>  <br>文档预处理（类似图书编目）  <br>  <br>把你的 PDF/Word 等文档拆成小段落（类似给每本书分章节）  <br>用嵌入模型将文字转成向量坐标（相当于给每本书贴上精确的地理坐标）  <br>存入向量数据库（相当于建立图书馆的索引系统）  <br>问答过程（类似图书检索）  <br>  <br>用户提问时，先将问题转成向量坐标  <br>在向量数据库里找坐标最近的文档段落（类似 GPS 定位最近的图书）  <br>只把相关段落喂给大模型（而不是整个图书馆）  <br>答案生成（像翻译官工作）  <br>  <br>大模型将专业文档"翻译"成人话  <br>结合找到的段落内容生成最终回答  <br>整个过程类似你给翻译官几页参考资料，让他帮忙解释某个问题  <br>关于 token 消耗的关键事实：  <br>  <br>预处理阶段（向量化）是单次成本  <br>每次问答的 token 消耗=提问长度+检索到的文档长度+回答长度  <br>  <br>相比直接微调大模型（需数万元成本），这种方案首次构建成本通常不超过千元，且支持动态更新文档。核心开发难点在于处理 PDF 解析和设计高效的检索策略，对熟悉 Web 开发的工程师来说，主要工作量在系统集成而非 AI 算法本身。|

|   |   |   |
|---|---|---|
|![zbw0414](https://cdn.v2ex.com/gravatar/8034ff7bc8476db0b02c2c1503ca2323?s=48&d=retro)||26<br><br>**[zbw0414](/member/zbw0414)**  <br><br>   3 小时 28 分钟前<br><br>说实话 ，你不如把你这些疑惑都去问 deepseek ，任何一丁点不清楚的都去问，比论坛这种你一言我一嘴的回答有意义的多了。|

|   |   |   |
|---|---|---|
|![logic159](https://cdn.v2ex.com/avatar/d9e5/544d/330204_normal.png?m=1711431966)||27<br><br>**[logic159](/member/logic159)**  <br><br>   2 小时 31 分钟前<br><br>最近这本书比较火，看看是不是有帮助  <br>[https://nndl.github.io/nndl-book.pdf](https://nndl.github.io/nndl-book.pdf)|

|   |   |   |
|---|---|---|
|![wxiao333](https://cdn.v2ex.com/gravatar/1b913ebf6075ecf33aaf67eff382ae7c?s=48&d=retro)||28<br><br>**[wxiao333](/member/wxiao333)**  <br><br>   38 分钟前<br><br>感觉用这种上下文会耗大量 token,这个确实是个问题啊。  <br>=======  <br>所以很多知识库的场景都是私有化部署，其实知识库很多时候 32b ，14b 的也够用了|

|   |   |   |
|---|---|---|
|![sm0king](https://cdn.v2ex.com/avatar/76c1/922d/16731_normal.png?m=1329507007)||29<br><br>**[sm0king](/member/sm0king)**  <br><br>   16 分钟前<br><br>@[tool2dx](/member/tool2dx) #17 比较费 token 的话，独立部署可以解决么？  <br>有个点没太明白，个人的知识库，应该不用那么多参数的 deepseek 模型就行吧（比如 7B ），这样是不是就可以解决 token 不够的问题了？|

|   |   |   |
|---|---|---|
|![pearce](https://cdn.v2ex.com/gravatar/8e88b0e973042ab826cf8bd0ee949b04?s=48&d=retro)||30<br><br>**[pearce](/member/pearce)**  <br><br>   4 分钟前<br><br>@[sm0king](/member/sm0king) 个人理解：比较费 token 的本质是费算力，本地部署使用的是自己的算力，只要你有足够的算力和电力那应该无所谓吧|
